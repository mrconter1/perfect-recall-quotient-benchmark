# 🧠 Perfect Recall Quotient (PRQ) Benchmark

PRQ Benchmark is a benchmark designed to measure the perfect recall capabilities of large language models. Unlike traditional benchmarks, PRQ focuses specifically on a model's ability to retrieve exact information from previously read scientific papers, providing a quantitative measure of an AI's memory and recall precision.

## 📊 Benchmark Results

<div align="center">

| Model                   | Percentage Success (%) |
|:-----------------------:|:------------------:|
| llama-3.1-70b-instruct  | 11.11              |
| claude-3.5-sonnet       | 5.56               |
| gpt-3.5-turbo           | 5.56               |
| llama-3.1-405b-instruct | 5.56               |
| mistral-large           | 5.56               |
| gemini-pro-1.5          | 0                  |
| claude-3-opus           | 0                  |
| gpt-4-turbo             | 0                  |
| gpt-4o                  | 0                  |
| gpt-4o-mini             | 0                  |
| llama-3.1-8b-instruct   | 0                  |

</div>

## 🚀 Getting Started

### 📋 Prerequisites

- Python 3.9+
- Access to AI models for testing

### 💻 Installation

1. Clone the repository:
   ```
   git clone https://github.com/yourusername/prq-benchmark
   cd prq-benchmark
   ```

2. Install required packages:
   ```
   pip install -r requirements.txt
   ```

### 🖥️ Usage

Run the main script:

```
python run_benchmark.py
```

## 👥 Contributing

Contributions are welcome! Please fork the project, create a feature branch, and open a pull request.

## 📄 License

Distributed under the MIT License. See `LICENSE` for more information.

## 📧 Contact

Rasmus Lindahl - rasmus.lindahl1996@gmail.com