# ğŸ§  Perfect Recall Quotient (PRQ) Benchmark

PRQ Benchmark is a benchmark designed to measure the perfect recall capabilities of large language models. Unlike traditional benchmarks, PRQ focuses specifically on a model's ability to retrieve exact information from previously read scientific papers, providing a quantitative measure of an AI's memory and recall precision.

## ğŸ“Š Benchmark Results

<div align="center">

| Model                   | Percentage Success (%) |
|:-----------------------:|:------------------:|
| llama-3.1-70b-instruct  | 11.11              |
| claude-3.5-sonnet       | 5.56               |
| gpt-3.5-turbo           | 5.56               |
| llama-3.1-405b-instruct | 5.56               |
| mistral-large           | 5.56               |
| gemini-pro-1.5          | 0                  |
| claude-3-opus           | 0                  |
| gpt-4-turbo             | 0                  |
| gpt-4o                  | 0                  |
| gpt-4o-mini             | 0                  |
| llama-3.1-8b-instruct   | 0                  |

</div>

## ğŸš€ Getting Started

### ğŸ“‹ Prerequisites

- Python 3.9+
- Access to AI models for testing

### ğŸ’» Installation

1. Clone the repository:
   ```
   git clone https://github.com/yourusername/prq-benchmark
   cd prq-benchmark
   ```

2. Install required packages:
   ```
   pip install -r requirements.txt
   ```

### ğŸ–¥ï¸ Usage

Run the main script:

```
python run_benchmark.py
```

## ğŸ‘¥ Contributing

Contributions are welcome! Please fork the project, create a feature branch, and open a pull request.

## ğŸ“„ License

Distributed under the MIT License. See `LICENSE` for more information.

## ğŸ“§ Contact

Rasmus Lindahl - rasmus.lindahl1996@gmail.com